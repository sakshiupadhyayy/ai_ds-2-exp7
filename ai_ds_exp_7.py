# -*- coding: utf-8 -*-
"""AI-DS_Exp-6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18KQT2oi0kaFPMLPNhwf5mlIt-jZgfiM5
"""

import pandas as pd
from numpy import mean
from numpy import std
from sklearn.ensemble import BaggingClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('data.csv')
df['gender'].replace('Female', 0,inplace=True)
df['gender'].replace('Male', 1,inplace=True)
df.head()

# evaluate bagging algorithm for classification
from numpy import mean
from numpy import std
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import BaggingClassifier
# define dataset
df = pd.read_csv('data.csv')
df['gender'].replace('Female', 0,inplace=True)
df['gender'].replace('Male', 1,inplace=True)
df.head()

model = BaggingClassifier()

x = df.iloc[:,:7]
x.shape

y = df.iloc[:,7:]
y.shape

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size =0.16)

x_train1= x_train.iloc[:1400,:]
x_train2= x_train.iloc[1401:2800,:]
x_train3= x_train.iloc[2801:4200,:]
x_train3.shape

y_train1= y_train.iloc[:1400,:]
y_train2= y_train.iloc[1401:2800,:]
y_train3= y_train.iloc[2801:4200,:]
y_train3.shape

y_pred1= modelLogistic.predict(x_test)
a=[]
b=[]
buffer = []
for index, row in df.iterrows():
  b.append(int(row['gender']))
for i in y_pred1:
  a.append(i)
for i, j in zip(a, b):
    if i != j:
        buffer.append(i)
print(buffer)

from sklearn.ensemble import RandomForestClassifier
from numpy import ravel
rd = RandomForestClassifier()

rd.fit(x_train2, y_train2.values.ravel())

y_pred2 = rd.predict(x_test)

print(accuracy_score(y_test, y_pred2))

from sklearn.naive_bayes import GaussianNB

# Build a Gaussian Classifier
model = GaussianNB()

# Model training
model.fit(x_train3, y_train3.values.ravel())
y_pred3 = model.predict(x_test)

print(accuracy_score(y_test, y_pred3))

from xgboost import XGBClassifier, plot_tree
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report

# Generate some sample data
np.random.seed(42)
X = np.random.rand(100, 7)
y = np.random.randint(2, size=100)

# Split the data into training and testing sets
x_train, x_test = X[:80], X[80:]
y_train, y_test = y[:80], y[80:]

# Train an XGBoost classifier
xgb = XGBClassifier()
xgb.fit(x_train, y_train)

# Predict on the test set
y_pred = xgb.predict(x_test)

print(classification_report(y_test, y_pred))

# Input to visualize
input_to_visualize = np.array([[1, 12, 5.8, 1, 1, 0, 1]])

# Plot the decision tree for the input
plt.figure(figsize=(30, 20))
plot_tree(xgb, num_trees=0, rankdir='LR', ax=plt.gca())
plt.show()

# Prepare manual entry data (replace this with your actual data)
import numpy as np
manual_entry = np.array([[1,12,5.8,1,1,0,1]])  # Make sure to match feature order and structure

# Make predictions
predictions = rd.predict(manual_entry)
if predictions == 1:
  print('Male')
else:
  print('Female')

